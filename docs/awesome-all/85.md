# Awesome LLM (å¤§å‹è¯­è¨€æ¨¡å‹)

ğŸ”¥ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å¸­å·äº† **å…¨çƒ**ï¼Œä¸å†å±€é™äº NLP æˆ– AI ç¤¾åŒºã€‚è¿™é‡Œæ•´ç†äº†ä¸€äº›å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ä¸ ChatGPT ç›¸å…³çš„ç ”ç©¶è®ºæ–‡ï¼Œæ¶µç›–äº† LLM è®­ç»ƒæ¡†æ¶ã€éƒ¨ç½²å·¥å…·ã€è¯¾ç¨‹ä¸æ•™ç¨‹ï¼Œä»¥åŠæ‰€æœ‰å…¬å¼€çš„ LLM æ£€æŸ¥ç‚¹å’Œ APIã€‚

## çƒ­é—¨ LLM é¡¹ç›®

- [Deep-Live-Cam](https://github.com/hacksider/Deep-Live-Cam) - åªéœ€ä¸€å¼ å›¾ç‰‡å³å¯å®ç°å®æ—¶æ¢è„¸å’Œä¸€é”®è§†é¢‘æ·±åº¦ä¼ªé€ ï¼ˆæœªç»è¿‡æ»¤ï¼‰ã€‚
- [MiniCPM-V 2.6](https://github.com/OpenBMB/MiniCPM-V) - ä¸€æ¬¾å¯ä»¥åœ¨æ‰‹æœºä¸Šä½¿ç”¨çš„ GPT-4V çº§åˆ«çš„ MLLMï¼Œæ”¯æŒå•å›¾ã€å¤šå›¾å’Œè§†é¢‘å¤„ç†ã€‚
- [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) - åªéœ€ 1 åˆ†é’Ÿçš„è¯­éŸ³æ•°æ®ï¼Œå°±èƒ½è®­ç»ƒå‡ºä¼˜ç§€çš„è¯­éŸ³åˆæˆæ¨¡å‹ï¼ï¼ˆå°‘æ ·æœ¬è¯­éŸ³å…‹éš†ï¼‰ã€‚


## é‡è¦è®ºæ–‡é‡Œç¨‹ç¢‘

|   æ—¥æœŸ   |      å…³é”®è¯      |     æœºæ„     |                                                                                              è®ºæ–‡                                                                                             |
|:--------:|:----------------:|:------------:|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2017-06  |     Transformers     |    Google    | [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)                                                                                                                           |
| 2018-06  |       GPT 1.0        |    OpenAI    | [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)                                                           |
| 2018-10  |         BERT         |    Google    | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf)                                                                     |
| 2019-02  |        GPT 2.0       |    OpenAI    | [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)                   |
| 2019-09  |     Megatron-LM      |    NVIDIA    | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)                                                                    |
| 2019-10  |         T5           |    Google    | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/v21/20-074.html)                                                                  |
| 2020-01  |    Scaling Law       |    OpenAI    | [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)                                                                                                                |
| 2020-05  |        GPT 3.0       |    OpenAI    | [Language models are few-shot learners](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)                                                                  |
| 2021-01  |  Switch Transformers |    Google    | [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)                                                           |
| 2021-08  |         Codex        |    OpenAI    | [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)                                                                                                        |
| 2021-08  |   Foundation Models  |   Stanford   | [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)                                                                                                   |
| 2021-09  |         FLAN         |    Google    | [Finetuned Language Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR)                                                                                                 |
| 2021-10  |          T0          | HuggingFace  | [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)                                                                                           |
| 2021-12  |         GLaM         |    Google    | [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf)                                                                                      |
| 2022-01  |          COT         |    Google    | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)                                                                                   |
| 2022-01  |         LaMDA        |    Google    | [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)                                                                                                          |
| 2022-04  |         PaLM         |    Google    | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)                                                                                                           |
| 2022-05  |          OPT         |     Meta     | [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)                                                                                                       |
| 2022-06  |  Emergent Abilities  |    Google    | [Emergent Abilities of Large Language Models](https://openreview.net/pdf?id=yzkSU5zdwD)                                                                                                         |
| 2022-10  |     Flan-T5/PaLM     |    Google    | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)                                                                                                          |
| 2022-11  |         BLOOM        | BigScience   | [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf)                                                                                         |
| 2022-12  |        OPT-IML       |     Meta     | [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/pdf/2212.12017)                                                                |
| 2023-01  | Flan 2022 Collection |    Google    | [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/pdf/2301.13688.pdf)                                                                       |
| 2023-02  |         LLaMA        |     Meta     | [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)                                       |
| 2023-03  |        PaLM-E        |    Google    | [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io)                                                                                                                       |
| 2023-03  |         GPT 4        |    OpenAI    | [GPT-4 Technical Report](https://openai.com/research/gpt-4)                                                                                                                                     |
| 2023-05  |       PaLM 2         |    Google    | [PaLM 2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf)                                                                                                              |
| 2023-07  |        LLaMA2        |     Meta     | [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)                                                                                                      |
| 2023-12  |         Mamba        | CMU&Princeton | [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752)                                                                                            |
| 2024-01  |         DeepSeek-v2   |    DeepSeek  | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)                                                                           |
| 2024-05  |         Mamba2       | CMU&Princeton | [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060)                                                      |

## å…¶ä»–ç›¸å…³è®ºæ–‡
å¦‚æœä½ å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸæ„Ÿå…´è¶£ï¼Œå¯ä»¥å‚è€ƒä¸Šé¢åˆ—å‡ºçš„é‡Œç¨‹ç¢‘è®ºæ–‡ï¼Œå¸®åŠ©ä½ äº†è§£å…¶å‘å±•å†ç¨‹å’Œå‰æ²¿åŠ¨æ€ã€‚ç„¶è€Œï¼Œæ¯ä¸ªæ–¹å‘çš„LLMéƒ½æœ‰å…¶ç‹¬ç‰¹çš„è§è§£å’Œè´¡çŒ®ï¼Œè¿™äº›å¯¹å…¨é¢ç†è§£è¯¥é¢†åŸŸè‡³å…³é‡è¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å­é¢†åŸŸçš„è¯¦ç»†è®ºæ–‡åˆ—è¡¨ï¼Œä¾›ä½ å‚è€ƒï¼š

- [LLMå¹»è§‰è®ºæ–‡æ±‡æ€»](https://github.com/LuckyyySTA/Awesome-LLM-hallucination) - æ”¶é›†å…³äºLLMå¹»è§‰çš„ç›¸å…³è®ºæ–‡ã€‚
- [LLMå¹»è§‰æ£€æµ‹è®ºæ–‡æ±‡æ€»](https://github.com/EdinburghNLP/awesome-hallucination-detection) - æ”¶é›†LLMå¹»è§‰æ£€æµ‹çš„ç›¸å…³ç ”ç©¶è®ºæ–‡ã€‚
- [LLMå®ç”¨æŒ‡å—](https://github.com/Mooler0410/LLMsPracticalGuide) - ç²¾é€‰çš„LLMå®ç”¨èµ„æºåˆ—è¡¨ã€‚
- [ChatGPTæç¤ºè¯­åˆé›†](https://github.com/f/awesome-chatgpt-prompts) - æ”¶é›†ç”¨äºChatGPTæ¨¡å‹çš„æç¤ºè¯­ç¤ºä¾‹ã€‚
- [ä¸­æ–‡ChatGPTæç¤ºè¯­åˆé›†](https://github.com/PlexPt/awesome-chatgpt-prompts-zh) - é€‚ç”¨äºChatGPTçš„ä¸­æ–‡æç¤ºè¯­ç¤ºä¾‹ã€‚
- [ChatGPTèµ„æºåˆé›†](https://github.com/humanloop/awesome-chatgpt) - æ”¶é›†æœ‰å…³ChatGPTå’ŒGPT-3çš„èµ„æºã€‚
- [æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰è®ºæ–‡æ±‡æ€»](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers) - æ¶µç›–â€œæ€ç»´é“¾æç¤ºå¼•å‘LLMæ¨ç†â€ç›¸å…³ç ”ç©¶ã€‚
- [æ·±åº¦æ¨ç†æç¤ºè¯­åˆé›†](https://github.com/logikon-ai/awesome-deliberative-prompting) - ä»‹ç»å¦‚ä½•ä½¿ç”¨æç¤ºè¯­å¼•å¯¼LLMè¿›è¡Œå¯é æ¨ç†å’Œå†³ç­–ã€‚
- [æŒ‡ä»¤è°ƒä¼˜è®ºæ–‡åˆé›†](https://github.com/SinclairCoder/Instruction-Tuning-Papers) - åŒ…å«â€œè‡ªç„¶æŒ‡ä»¤â€ï¼ˆACL 2022ï¼‰ã€â€œFLANâ€ï¼ˆICLR 2022ï¼‰å’Œâ€œT0â€ï¼ˆICLR 2022ï¼‰ç­‰æŒ‡ä»¤è°ƒä¼˜ç›¸å…³è®ºæ–‡ã€‚
- [LLMé˜…è¯»æ¸…å•](https://github.com/crazyofapple/Reading_groups/) - å¤§è¯­è¨€æ¨¡å‹ç›¸å…³è®ºæ–‡å’Œèµ„æºæ±‡æ€»ã€‚
- [è¯­è¨€æ¨¡å‹æ¨ç†ç ”ç©¶](https://github.com/atfortes/LM-Reasoning-Papers) - è¯­è¨€æ¨¡å‹æ¨ç†ç›¸å…³çš„è®ºæ–‡å’Œèµ„æºåˆé›†ã€‚
- [æ€ç»´é“¾æ¨ç†å¹³å°](https://github.com/FranxYao/chain-of-thought-hub) - è¡¡é‡LLMæ¨ç†æ€§èƒ½çš„ç›¸å…³èµ„æºã€‚
- [GPTèµ„æºåˆé›†](https://github.com/formulahendry/awesome-gpt) - ä¸GPTã€ChatGPTã€OpenAIã€LLMç›¸å…³çš„ä¼˜è´¨é¡¹ç›®å’Œèµ„æºã€‚
- [GPT-3èµ„æºåˆé›†](https://github.com/elyase/awesome-gpt3) - æ”¶é›†å…³äºOpenAI GPT-3 APIçš„æ¼”ç¤ºå’Œæ–‡ç« ã€‚
- [LLMäººç±»åå¥½æ•°æ®é›†åˆé›†](https://github.com/PolisAI/awesome-llm-human-preference-datasets) - ä¾›LLMæŒ‡ä»¤è°ƒä¼˜ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’Œè¯„ä¼°ä½¿ç”¨çš„äººç±»åå¥½æ•°æ®é›†ã€‚
- [RWKVæ•™ç¨‹](https://github.com/Hannibal046/RWKV-howto) - RWKVå­¦ä¹ ç›¸å…³ææ–™å’Œæ•™ç¨‹ã€‚
- [å¤§è¯­è¨€æ¨¡å‹ç¼–è¾‘è®ºæ–‡æ±‡æ€»](https://github.com/zjunlp/ModelEditingPapers) - å…³äºå¤§è¯­è¨€æ¨¡å‹ç¼–è¾‘çš„è®ºæ–‡å’Œèµ„æºåˆé›†ã€‚
- [LLMå®‰å…¨èµ„æºåˆé›†](https://github.com/corca-ai/awesome-llm-security) - ä¸LLMå®‰å…¨ç›¸å…³çš„å·¥å…·ã€æ–‡æ¡£å’Œé¡¹ç›®èµ„æºã€‚
- [LLMä¸äººç±»å¯¹é½ç ”ç©¶èµ„æºæ±‡æ€»](https://github.com/GaryYufei/AlignLLMHumanSurvey) - å…³äºå¤§è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹é½çš„è®ºæ–‡å’Œèµ„æºé›†åˆã€‚
- [ä»£ç LLMèµ„æºåˆé›†](https://github.com/huybery/Awesome-Code-LLM) - æ”¶é›†ä¸ä»£ç LLMç›¸å…³çš„ç ”ç©¶å’Œèµ„æºã€‚
- [LLMå‹ç¼©è®ºæ–‡åˆé›†](https://github.com/HuangOwen/Awesome-LLM-Compression) - å…³äºLLMå‹ç¼©çš„ç ”ç©¶è®ºæ–‡å’Œå·¥å…·ã€‚
- [LLMç³»ç»Ÿç ”ç©¶è®ºæ–‡åˆé›†](https://github.com/AmberLJC/LLMSys-PaperList) - ç ”ç©¶LLMç³»ç»Ÿçš„ç›¸å…³è®ºæ–‡é›†åˆã€‚
- [LLMåº”ç”¨WebAppèµ„æºåˆé›†](https://github.com/snowfort-ai/awesome-llm-webapps) - æ”¶é›†å¼€æºå¹¶ç§¯æç»´æŠ¤çš„LLMåº”ç”¨Webåº”ç”¨ã€‚
- [æ—¥æœ¬è¯­LLMèµ„æºæ±‡æ€»](https://github.com/llm-jp/awesome-japanese-llm) - æ—¥æœ¬è¯­LLMçš„æ¦‚è§ˆã€‚
- [LLMåœ¨åŒ»ç–—ä¸­çš„åº”ç”¨è®ºæ–‡æ±‡æ€»](https://github.com/mingze-yuan/Awesome-LLM-Healthcare) - å…³æ³¨LLMåœ¨åŒ»å­¦é¢†åŸŸåº”ç”¨çš„ç›¸å…³è®ºæ–‡ã€‚
- [LLMæ¨ç†è®ºæ–‡åˆé›†](https://github.com/DefTruth/Awesome-LLM-Inference) - ä»¥æ¨ç†ä¸ºä¸»é¢˜çš„LLMç›¸å…³è®ºæ–‡æ±‡æ€»ã€‚
- [3Dä¸–ç•Œä¸­çš„LLMç ”ç©¶èµ„æº](https://github.com/ActiveVisionLab/Awesome-LLM-3D) - ç ”ç©¶3Dä¸–ç•Œä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è®ºæ–‡å’Œèµ„æºåˆé›†ã€‚
- [LLMè®­ç»ƒæ•°æ®é›†åˆé›†](https://github.com/Zjh-819/LLMDataHub) - ä¸“ä¸ºèŠå¤©æœºå™¨äººè®­ç»ƒè®¾è®¡çš„LLMæ•°æ®é›†ï¼ŒåŒ…å«æ•°æ®é›†é“¾æ¥ã€å¤§å°ã€è¯­è¨€ã€ç”¨é€”å’Œç®€è¦æè¿°ã€‚
- [ä¸­æ–‡LLMèµ„æºåˆé›†](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM) - æ±‡æ€»å¼€æºä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å°è§„æ¨¡ã€å¯ç§æœ‰åŒ–éƒ¨ç½²ã€è®­ç»ƒæˆæœ¬è¾ƒä½çš„æ¨¡å‹åŠå…¶åº”ç”¨ã€æ•°æ®é›†å’Œæ•™ç¨‹ç­‰ã€‚
- [LLMä¼˜åŒ–ä»»åŠ¡ç ”ç©¶åˆé›†](https://github.com/FeiLiu36/LLM4Opt) - æ¢ç´¢LLMåœ¨ä¼˜åŒ–ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæ”¶é›†ç›¸å…³ç ”ç©¶è®ºæ–‡ã€‚
- [è¯­è¨€æ¨¡å‹åˆ†æè®ºæ–‡åˆé›†](https://github.com/Furyton/awesome-language-model-analysis) - èšç„¦äºè¯­è¨€æ¨¡å‹çš„ç†è®ºå’Œå®è¯åˆ†æï¼Œæ¶‰åŠå­¦ä¹ åŠ¨æ€ã€è¡¨ç°åŠ›ã€å¯è§£é‡Šæ€§ã€æ³›åŒ–èƒ½åŠ›ç­‰è®®é¢˜ã€‚

## å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ’è¡Œæ¦œ

ä»¥ä¸‹æ˜¯ä¸€äº›è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ’è¡Œæ¦œå¹³å°å’ŒåŸºå‡†ï¼Œæ¶µç›–äº†ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ï¼Œä¾›å¼€å‘è€…å’Œç ”ç©¶è€…å‚è€ƒï¼š

- **Chatbot Arena Leaderboard** - [Hugging Faceå¹³å°](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)ï¼Œé€šè¿‡åŒ¿åå’Œéšæœºå¯¹æˆ˜çš„æ–¹å¼ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èŠå¤©ä¸­çš„è¡¨ç°ã€‚
- **Open LLM Leaderboard** - [Hugging Faceå¹³å°](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)ï¼Œä¸“é—¨è·Ÿè¸ªã€æ’åå’Œè¯„ä¼°å‘å¸ƒçš„LLMså’ŒèŠå¤©æœºå™¨äººã€‚
- **Chinese Large Model Leaderboard** - [ä¸­æ–‡LLMæ’è¡Œæ¦œ](https://github.com/jeinlee1991/chinese-llm-benchmark)ï¼Œä¸“é—¨è¯„ä¼°ä¸­æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- **CompassRank** - [CompassRankå¹³å°](https://rank.opencompass.org.cn)ï¼Œè¯„ä¼°è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„è¡¨ç°ï¼Œæä¾›ä¸€ä¸ªå…¨é¢ã€å…¬æ­£çš„è¡Œä¸šå‚è€ƒã€‚
- **InfiBench** - [InfiBenchå¹³å°](https://infi-coder.github.io/infibench)ï¼Œä¸“æ³¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å®é™…ç¼–ç¨‹é—®é¢˜ä¸Šçš„èƒ½åŠ›ã€‚
- **LawBench** - [æ³•å¾‹é¢†åŸŸè¯„æµ‹å¹³å°](https://lawbench.opencompass.org.cn/leaderboard)ï¼Œè¯„ä¼°LLMsåœ¨æ³•å¾‹é¢†åŸŸçš„è¡¨ç°ã€‚
- **MathEval** - [MathEvalå¹³å°](https://matheval.ai)ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é¢†åŸŸçš„èƒ½åŠ›ï¼Œæ¶µç›–20ä¸ªé¢†åŸŸå’Œè¿‘30,000ä¸ªæ•°å­¦é—®é¢˜ã€‚
- **MixEval** - [MixEvalå¹³å°](https://mixeval.github.io/#leaderboard)ï¼ŒåŸºäºçœŸå®æ•°æ®çš„åŠ¨æ€è¯„æµ‹å¹³å°ï¼Œè¯„ä¼°LLMsåœ¨æ··åˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿è¡Œé«˜æ•ˆã€æˆæœ¬ä½ã€‚
- **SuperBench** - [SuperBenchå¹³å°](https://fm.ai.tsinghua.edu.cn/superbench/#/leaderboard)ï¼Œä¸€ä¸ªç»¼åˆæ€§è¯„æµ‹å¹³å°ï¼Œè¯„ä¼°LLMsåœ¨è‡ªç„¶è¯­è¨€ç†è§£ã€æ¨ç†å’Œæ³›åŒ–ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
- **OlympicArena** - [å­¦æœ¯é¢†åŸŸè¯„æµ‹å¹³å°](https://gair-nlp.github.io/OlympicArena/#leaderboard)ï¼Œæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©å­¦ç­‰å¤šä¸ªå­¦ç§‘ã€‚
- **We-Math** - [We-Mathå¹³å°](https://we-math.github.io/#leaderboard)ï¼Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸Šçš„èƒ½åŠ›ã€‚

### å¼€æºå¤§è¯­è¨€æ¨¡å‹ (LLM)

#### Meta
- [Llama 3 ç³»åˆ—](https://llama.meta.com/)
- [OPT ç³»åˆ—](https://arxiv.org/abs/2205.01068)

#### Mistral AI
- [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/)
- [Mixtral ç³»åˆ—](https://mistral.ai/news/mixtral-of-experts/)

#### Google
- [Gemma ç³»åˆ—](https://blog.google/technology/developers/google-gemma-2/)
- [T5](https://arxiv.org/abs/1910.10683)

#### Apple
- [OpenELM-1.1](https://huggingface.co/apple/OpenELM)

#### Microsoft
- [Phi ç³»åˆ—](https://huggingface.co/microsoft/phi-1)

#### Cohere
- [Command R-35B](https://huggingface.co/CohereForAI/c4ai-command-r-v01)

#### DeepSeek
- [DeepSeek ç³»åˆ—](https://huggingface.co/collections/deepseek-ai)

#### Alibaba
- [Qwen ç³»åˆ—](https://huggingface.co/collections/Qwen/qwen-65c0e50c3f1ab89cb8704144)

#### Baichuan
- [Baichuan ç³»åˆ—](https://huggingface.co/baichuan-inc)

#### Nvidia
- [Nemotron-4-340B](https://huggingface.co/nvidia/Nemotron-4-340B-Instruct)

#### Zhipu AI
- [GLM-2 ç³»åˆ—](https://huggingface.co/THUDM)

#### Stability AI
- [StableLM ç³»åˆ—](https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a)

#### DataBricks
- [MPT-7B](https://www.databricks.com/blog/mpt-7b)

#### ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤
- [InternLM ç³»åˆ—](https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c)

### LLM æ•°æ®
- [LLMDataHub](https://github.com/Zjh-819/LLMDataHub)
- [IBM æ•°æ®é¢„å¤„ç†å·¥å…·åŒ…](https://github.com/IBM/data-prep-kit) - é«˜æ•ˆå¤„ç†éç»“æ„åŒ–æ•°æ®çš„å¼€æºå·¥å…·åŒ…ã€‚

### LLM è¯„ä¼°å·¥å…·
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [MixEval](https://github.com/Psycoy/MixEval)
- [lighteval](https://github.com/huggingface/lighteval)
- [OLMO-eval](https://github.com/allenai/OLMo-Eval)
- [instruct-eval](https://github.com/declare-lab/instruct-eval)
- [simple-evals](https://github.com/openai/simple-evals)
- [Giskard](https://github.com/Giskard-AI/giskard)
- [LangSmith](https://www.langchain.com/langsmith)
- [Ragas](https://github.com/explodinggradients/ragas)

## LLMè®­ç»ƒæ¡†æ¶

- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - ä¸€æ¬¾æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ï¼Œæé«˜æ•ˆç‡å’Œæ•ˆæœã€‚
- [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed) - NVIDIA Megatron-LMçš„DeepSpeedç‰ˆæœ¬ï¼Œå¢å¼ºäº†å¯¹MoEæ¨¡å‹è®­ç»ƒã€è¯¾ç¨‹å­¦ä¹ ã€3Då¹¶è¡Œç­‰ç‰¹æ€§çš„æ”¯æŒã€‚
- [torchtune](https://github.com/pytorch/torchtune) - PyTorchåŸç”Ÿåº“ï¼Œç”¨äºå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒã€‚
- [NeMo Framework](https://github.com/NVIDIA/NeMo) - NVIDIAæ¨å‡ºçš„ç”Ÿæˆå¼AIæ¡†æ¶ï¼Œæ”¯æŒLLMã€è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç­‰å¤šä¸ªé¢†åŸŸçš„ç ”ç©¶ã€‚
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - è¿›è¡Œå¤§è§„æ¨¡Transformeræ¨¡å‹è®­ç»ƒçš„ç ”ç©¶æ¡†æ¶ã€‚
- [Colossal-AI](https://github.com/hpcaitech/ColossalAI) - è®©å¤§å‹AIæ¨¡å‹è®­ç»ƒå˜å¾—æ›´ä¾¿å®œã€æ›´é«˜æ•ˆã€æ›´æ˜“è®¿é—®ã€‚
- [BMTrain](https://github.com/OpenBMB/BMTrain) - é«˜æ•ˆçš„å¤§å‹æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚
- [Mesh TensorFlow](https://github.com/tensorflow/mesh) - æä¾›ä¾¿æ·çš„æ¨¡å‹å¹¶è¡ŒåŒ–è®­ç»ƒæ–¹æ¡ˆã€‚
- [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) - åŸºäºDeepSpeedåº“çš„GPUå¹¶è¡Œè‡ªå›å½’Transformeræ¨¡å‹å®ç°ã€‚

## LLMéƒ¨ç½²

> å‚è€ƒï¼š[llm-inference-solutions](https://github.com/mani-kantap/llm-inference-solutions)
- [SGLang](https://github.com/sgl-project/sglang) - é«˜æ•ˆçš„LLMå’Œè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ã€‚
- [vLLM](https://github.com/vllm-project/vllm) - é«˜ååã€ä½å†…å­˜æ¶ˆè€—çš„LLMæ¨ç†å’ŒæœåŠ¡å¼•æ“ã€‚
- [TGI](https://huggingface.co/docs/text-generation-inference/en/index) - Hugging Faceæ¨å‡ºçš„LLMéƒ¨ç½²å’ŒæœåŠ¡å·¥å…·åŒ…ã€‚
- [exllama](https://github.com/turboderp/exllama) - ä¸ºé‡åŒ–æƒé‡çš„Llamaæ¨¡å‹æä¾›çš„æ›´é«˜æ•ˆå†…å­˜ç‰ˆæœ¬ã€‚
- [FastChat](https://github.com/lm-sys/FastChat) - æ”¯æŒå¤šç§æ¨¡å‹çš„åˆ†å¸ƒå¼LLMæœåŠ¡ç³»ç»Ÿï¼Œæä¾›Web UIå’ŒOpenAIå…¼å®¹çš„RESTful APIã€‚
- [LangChain](https://github.com/hwchase17/langchain) - ç”¨äºæ„å»ºåŸºäºLLMçš„åº”ç”¨çš„Python/JavaScriptåº“ï¼Œæ”¯æŒé€šè¿‡ç»„åˆæ¨¡å‹å®ç°å¤æ‚åº”ç”¨ã€‚

## LLMåº”ç”¨

- [MLflow](https://mlflow.org/) - å¼€æºæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†å¹³å°ï¼Œæ”¯æŒå®éªŒè·Ÿè¸ªã€æ¨¡å‹è¯„ä¼°å’Œéƒ¨ç½²ã€‚
- [YiVal](https://github.com/YiVal/YiVal) - å¼€æºçš„GenAI-Opså·¥å…·ï¼Œç”¨äºè°ƒä¼˜å’Œè¯„ä¼°LLMæ¨¡å‹çš„æç¤ºã€é…ç½®åŠæ¨¡å‹å‚æ•°ã€‚
- [LangChain](https://github.com/hwchase17/langchain) - ç”¨äºæ„å»ºLLMé“¾å¼åº”ç”¨çš„æµè¡ŒPythonåº“ã€‚
- [Prompttools](https://github.com/hegelai/prompttools) - ç”¨äºæµ‹è¯•å’Œè¯„ä¼°æ¨¡å‹ã€å‘é‡æ•°æ®åº“åŠæç¤ºçš„å¼€æºå·¥å…·é›†ã€‚
- [Weights & Biases](https://wandb.ai/site/solutions/llmops) - ç”¨äºè·Ÿè¸ªæ¨¡å‹è®­ç»ƒå’Œæç¤ºä¼˜åŒ–å®éªŒçš„å•†ä¸šå·¥å…·ã€‚

## LLMæ•™ç¨‹ä¸è¯¾ç¨‹

- [LLMè¯¾ç¨‹](https://github.com/mlabonne/llm-course) - å¸¦æœ‰è·¯çº¿å›¾å’ŒColabç¬”è®°æœ¬ï¼Œå¸®åŠ©ä½ æ·±å…¥äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚
- [UWaterloo CS 886](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/) - æ¢ç´¢åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚
- [CS25-Transformers United](https://web.stanford.edu/class/cs25/) - æ–¯å¦ç¦å¤§å­¦çš„Transformersè¯¾ç¨‹ã€‚
- [ChatGPTæç¤ºå·¥ç¨‹](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) - ä¸“ä¸ºå¼€å‘è€…è®¾è®¡çš„ChatGPTæç¤ºå·¥ç¨‹è¯¾ç¨‹ã€‚
- [æ™®æ—æ–¯é¡¿å¤§å­¦ï¼šç†è§£å¤§å‹è¯­è¨€æ¨¡å‹](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/) - æ™®æ—æ–¯é¡¿å¤§å­¦å…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯¾ç¨‹ã€‚
- [CS324 - å¤§å‹è¯­è¨€æ¨¡å‹](https://stanford-cs324.github.io/winter2022/) - æ–¯å¦ç¦å¤§å­¦çš„LLMè¯¾ç¨‹ã€‚
- [GPTçŠ¶æ€åˆ†æ](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2) - äº†è§£GPTæ¨¡å‹çš„æœ€æ–°å‘å±•ã€‚
- [æ„å»ºGPTï¼šä»é›¶å¼€å§‹çš„ä»£ç å®ç°](https://www.youtube.com/watch?v=kCc8FmEb1nY) - ä¸€éƒ¨è¯¦å°½çš„GPTä»é›¶å®ç°æ•™ç¨‹ã€‚
- [BPEæœ€ç®€å®ç°](https://www.youtube.com/watch?v=zduSFxRajkE&t=1157s) - ä»‹ç»å¸¸ç”¨äºLLMæ ‡è®°åŒ–çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•ã€‚
- [femtoGPT](https://github.com/keyvank/femtoGPT) - ä½¿ç”¨çº¯Rustå®ç°çš„æœ€ç®€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰ã€‚
- [Neurips2022ï¼šåŸºç¡€æ¨¡å‹çš„é²æ£’æ€§](https://nips.cc/virtual/2022/tutorial/55796) - è®¨è®ºåŸºç¡€æ¨¡å‹çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚
- [ICML2022ï¼šå¤§æ¨¡å‹æ—¶ä»£çš„æŠ€æœ¯ä¸ç³»ç»Ÿ](https://icml.cc/virtual/2022/tutorial/18440) - æ¢ç´¢å¤§æ¨¡å‹çš„è®­ç»ƒä¸åº”ç”¨ã€‚
- [GPT60è¡Œä»£ç å®ç°](https://jaykmody.com/blog/gpt-from-scratch/) - ç”¨60è¡ŒNumPyä»£ç å®ç°ä¸€ä¸ªç®€å•çš„GPTã€‚

## LLMä¹¦ç±æ¨è

- [ã€ŠLangChainä¸ç”Ÿæˆå¼AIã€‹](https://amzn.to/3GUlRng) - æœ¬ä¹¦å±•ç¤ºå¦‚ä½•ä½¿ç”¨Pythonã€ChatGPTåŠå…¶ä»–LLMæ„å»ºç”Ÿæˆå¼AIåº”ç”¨ï¼Œå¹¶é™„å¸¦[GitHubä»£ç ](https://github.com/benman1/generative_ai_with_langchain)ã€‚
- [ã€Šä»é›¶å¼€å§‹æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ã€‹](https://www.manning.com/books/build-a-large-language-model-from-scratch) - è¯¦ç»†æŒ‡å¯¼å¦‚ä½•æ„å»ºä¸€ä¸ªå¯ç”¨çš„LLMã€‚
- [ã€Šæ„å»ºGPTï¼šå¦‚ä½•å·¥ä½œã€‹](https://www.amazon.com/dp/9152799727?ref_=cm_sw_r_cp_ud_dp_W3ZHCD6QWM3DPPC0ARTT_1) - ä»é›¶å¼€å§‹è®²è§£å¦‚ä½•ç¼–å†™ä¸€ä¸ªç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰ã€‚
- [ã€Šå¤§å‹è¯­è¨€æ¨¡å‹å®æˆ˜ã€‹](https://www.llm-book.com/) - ä¸€æœ¬è¯¦ç»†çš„å›¾è§£ä¹¦ç±ï¼Œå¸¦ä½ æ·±å…¥äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶åº”ç”¨ã€‚

## LLMç›¸å…³æ€è€ƒ

- [ä¸ºä»€ä¹ˆæ‰€æœ‰GPT-3çš„å…¬å¼€å¤ç°éƒ½å¤±è´¥äº†ï¼Ÿ](https://jingfengyang.github.io/gpt)
- [æŒ‡ä»¤è°ƒä¼˜çš„é˜¶æ®µæ€§å›é¡¾](https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2)
- [LLMé©±åŠ¨çš„è‡ªä¸»ä»£ç†](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [ä¸ºä»€ä¹ˆä½ åº”è¯¥ä»äº‹AIä»£ç†çš„å·¥ä½œï¼](https://www.youtube.com/watch?v=fqVLjtvWgq8)
- [è°·æ­Œï¼šæˆ‘ä»¬æ²¡æœ‰æŠ¤åŸæ²³ï¼ŒOpenAIä¹Ÿæ²¡æœ‰](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
- [AIç«äº‰å£°æ˜](https://petergabriel.com/news/ai-competition-statement/)
- [æç¤ºå·¥ç¨‹æ¦‚è¿°](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
- [ä¹”å§†æ–¯åŸºï¼šChatGPTçš„è™šå‡æ‰¿è¯º](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html)
- [ChatGPTçš„1750äº¿å‚æ•°ï¼šæŠ€æœ¯åˆ†æ](https://orenleung.super.site/is-chatgpt-175-billion-parameters-technical-analysis)
- [å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä»£](https://www.notion.so/Awesome-LLM-40c8aa3f2b444ecc82b79ae8bbd2696b)
- [2023å¹´å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒ](https://research.aimultiple.com/large-language-model-training/)
- [GPTå¦‚ä½•è·å¾—å…¶èƒ½åŠ›ï¼Ÿè¿½æº¯è¯­è¨€æ¨¡å‹çš„æ¶Œç°èƒ½åŠ›](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)

## å…¶ä»–èµ„æº

- [Arize-Phoenix](https://phoenix.arize.com/) - ç”¨äºæœºå™¨å­¦ä¹ å¯è§‚å¯Ÿæ€§çš„å¼€æºå·¥å…·ï¼Œæ”¯æŒåœ¨ä½ çš„ç¬”è®°æœ¬ç¯å¢ƒä¸­è¿è¡Œå¹¶è°ƒæ•´LLMã€è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å’Œè¡¨æ ¼æ•°æ®æ¨¡å‹ã€‚
- [Emergent Mind](https://www.emergentmind.com) - æœ€æ–°çš„AIæ–°é—»ï¼Œç”±GPT-4è§£æå’Œè§£é‡Šã€‚
- [ShareGPT](https://sharegpt.com) - ä¸€é”®åˆ†äº«ä½ ä¸ChatGPTçš„å¯¹è¯ã€‚
- [ä¸»è¦LLMåŠæ•°æ®å¯ç”¨æ€§](https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0) - ä¸»è¦LLMæ¨¡å‹çš„æ¦‚è§ˆåŠå…¶æ•°æ®å¯ç”¨æ€§ã€‚
- [500+æœ€ä½³AIå·¥å…·](https://vaulted-polonium-23c.notion.site/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126)
- [Cohere Summarize Beta](https://txt.cohere.ai/summarize-beta/) - Cohereæ¨å‡ºçš„æ–‡æœ¬æ‘˜è¦APIã€‚
- [chatgpt-wrapper](https://github.com/mmabrouk/chatgpt-wrapper) - ä¸€ä¸ªå¼€æºçš„Python APIå’ŒCLIå·¥å…·ï¼Œç”¨äºä¸ChatGPTäº¤äº’ã€‚
- [Open-evals](https://github.com/open-evals/evals) - ç”¨äºä¸åŒè¯­è¨€æ¨¡å‹è¯„ä¼°çš„æ‰©å±•æ¡†æ¶ã€‚
- [Cursor](https://www.cursor.so) - ä¸€ä¸ªå¼ºå¤§çš„AIå·¥å…·ï¼Œç”¨äºç¼–å†™ã€ç¼–è¾‘å’Œè®¨è®ºä»£ç ã€‚
- [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) - ä¸€ä¸ªå±•ç¤ºGPT-4åŠŸèƒ½çš„å¼€æºåº”ç”¨ã€‚
- [OpenAGI](https://github.com/agiresearch/OpenAGI) - å½“LLMé‡åˆ°é¢†åŸŸä¸“å®¶æ—¶ã€‚
- [EasyEdit](https://github.com/zjunlp/EasyEdit) - ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„æ¡†æ¶ï¼Œç”¨äºç¼–è¾‘å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- [chatgpt-shroud](https://github.com/guyShilo/chatgpt-shroud) - ä¸€ä¸ªChromeæ‰©å±•ï¼Œç”¨äºä¿æŠ¤ç”¨æˆ·éšç§ï¼Œå…è®¸è½»æ¾éšè—å’Œæ˜¾ç¤ºChatGPTèŠå¤©è®°å½•ã€‚é€‚åˆå±å¹•å…±äº«æ—¶ä½¿ç”¨ã€‚
